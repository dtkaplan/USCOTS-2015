---
title: "USCOTS data science workshop: intro to text mining"
author: "Nicholas Horton (nhorton@amherst.edu)"
date: "May 25, 2015"
output: 
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---


```{r include=FALSE}
# Don't delete this chunk if you are using the mosaic package
# This loads the mosaic and dplyr packages
require(mosaic)
require(DCF)
```

```{r include=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).

# This changes the default colors in lattice plots.
trellis.par.set(theme=theme.mosaic())  

# knitr settings to control how R chunks work.
require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small"    # slightly smaller font for code
)
# This loads the mosaic data sets.  (Could be deleted if you are not using them.)
require(mosaicData)                
```

### Introduction

Text mining (a form of text analytics) is a fast-growing application of statistical and machine learning techniques.  There are a number of R packages that facilitate
analysis of text documents.
The `aRxiv` package facilitates access to http://arXiv.org, a repository
of electronic preprints for a number of scientific disciplines.  It receives
many thousands of new submissions each month.
In this example, we utilize functions from the `tm` package to
generate a corpus of documents (a structured
set of texts) consisting of the abstracts from a search of papers on
http://arxiv.org.

In this markdown file, we will explore some 
analyses of the text in some of these abstracts.

To avoid overloading the arXiv server, we will use data that have been previously downloaded using the following commands:
```{r eval=FALSE}
require(aRxiv); require(lubridate); require(stringr)
papers <- arxiv_search(query='cat:stat*', limit=1000)
papers <- mutate(papers, submityear =
  year(sapply(str_split(submitted, " "), "[[", 1)))
save(papers, file="arXivpapers2015-05.Rda")
```

Download the dataframe with the papers
```{r}
filename <- "arXivpapers2015-05.Rda"
if (!file.exists(filename)) {
  download.file("http://dtkaplan.github.io/USCOTS-2015/Handouts/arXivpapers2015-05.Rda", filename)
}
load(filename)
```

```{r}
names(papers)
glimpse(papers)
tally(~ submityear, data=papers)
```

```{r}
# what has Brad Efron been up to?
efron <- filter(papers, grepl("Efron", authors))
efron
```


```{r, message=FALSE}
# let's use the text mining package to create a corpus
require(tm)
mycorpus <- VCorpus(DataframeSource(data.frame(papers$abstract)))
head(strwrap(mycorpus[[1]]))
```

Next we want to clean up the corpus.
The `tm_map()` function is called repeatedly to strip whitespace,
remove numbers and punctuation, map all of the text to lowercase, and
elide common English words.

```{r}
origcorpus <- mycorpus
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
head(strwrap(mycorpus[[1]]))
```

Let's generate a wordcloud...
```{r warning=FALSE}
require(wordcloud)
wordcloud(mycorpus, colors=brewer.pal(6, "Dark2"), random.order=FALSE)
```




Finally, the `DocumentTermMatrix()` can be used to generate a document
term matrix.  This is a sparse matrix that describes the frequency of terms
in a corpus.  We can display the terms which arise in 100 or more of the abstracts.

```{r}
dtm <- DocumentTermMatrix(mycorpus)
findFreqTerms(dtm, 100)
```

```{r}
# what words are associated with "markov"?
findAssocs(dtm, "markov", 0.5)
```

```{r}
# what words are associated with "regression"?
findAssocs(dtm, "regression", 0.4)
```


```{r}
origcorpus[[7]]
```

```{r}
inspect(DocumentTermMatrix(mycorpus, list(dictionary=c("bayesian", "markov", "bootstrap"))))
```


>>>>>>> 4eaafd5eb990f78d8d61d859000703f6b8b1ca78
